<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SnoutCounter - About</title>
    <link rel="shortcut icon" type="image/ico" href="assets/images/snoutcounter-favicon.ico"/>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <script src="https://cdn.jsdelivr.net/gh/ncase/nutshell/nutshell.js"></script>
    <script type="text/javascript"
        src="https://d3eoax9i5htok0.cloudfront.net/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    <script>
        Nutshell.setOptions({
            startOnLoad: true, // Start Nutshell on load? (default: true)
            lang: 'en', // Language (default: 'en', which is English)
            dontEmbedHeadings: true, // If 'true', removes the "embed this as a nutshell" option on headings
        });
    </script>
    <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:ital,wght@0,400;0,700;1,400;1,700&display=swap" rel="stylesheet">
    <style>
        .nav-link:hover{
            text-decoration: none;
        }
        @font-face {
            font-family: 'California Gothic';
            src: url('assets/fonts/CaliforniaGothic-Regular.ttf') format('truetype');
            font-weight: normal;
            font-style: normal;
        }
        /* @font-face{
            font-family: 'IBM Plex Sans';
            src: url('assets/fonts/IBMPlexSans-VariableFont_wdth,wght.ttf') format('truetype');
            font-weight: normal;
            font-style: normal;
        } */
        h1 {
            font-family: 'California Gothic', sans-serif;
        }
        h2 {
            font-family: 'California Gothic', sans-serif;
        }
        p {
            font-family: 'IBM Plex Sans', sans-serif;
        }
        ul {
            font-family: 'IBM Plex Sans', sans-serif;
        }
        a:not(.nav-link):link {
            color: #E85D20;
        }
        a:not(.nav-link):visited {
            color: #d4433b;
        }
        a:not(.nav-link):hover {
            color: #E85D20;
            background-color: #fcebe6;
        }
        a:not(.nav-link):active {
            color: #A1370D;
        }
        .fancy-underline {
            position: relative;
            display: inline-block;
            font-family: 'IBM Plex Sans', sans-serif;
            font-weight: 400;
            text-decoration: none;
            color: #333;
            padding-bottom: 4px;
            transition: color 0.3s ease;
        }

        .fancy-underline::after {
            content: '';
            position: absolute;
            width: 0%;
            height: 2px;
            left: 0;
            bottom: 0;
            background-color: #007bff;
            transition: width 0.3s ease;
        }

        .fancy-underline:hover::after {
            width: 100%;
            background-color: none;
        }

        .fancy-underline:hover {
            color: #007bff;
            background-color: none;
        }
    </style>
</head>
<body>
    <nav class="navbar navbar-expand-lg navbar-light bg-white shadow-sm">
    <div class="container">
        <a class="navbar-brand" href="index.html"><img src="assets/images/snoutcounter.png" height="75" alt="Logo"></a>
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav"
            aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarNav">
            <ul class="navbar-nav ms-auto">
                <li class="nav-item">
                    <a class="nav-link fancy-underline" href="index.html">HOME</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link fancy-underline" href="#">ABOUT</a>
                </li>
            </ul>
        </div>
    </div>
</nav>
    <br><br>
    <div class="container">
        <!-- <figure class="quote"><blockquote>“Some of these not-empires use snoutcounting to confer legitimacy on leaders in the same way we use the imperial succession,” Kirel answered. “This may tend to minimize the disruption that will arise in the United States as a result of the loss of Roosevelt.”</blockquote></figure>
        <p class="text-center">- Harry Turtledove, <i>Worldwar: Striking the Balance</i></p> -->
        <p class="text-left" style="margin-left:10%; margin-right:10%">SnoutCounter is a poll aggregation site compiling polling averages on figure approval, favorability, and electoral intent. It currently tracks presidential approval (including approval on specific issues). I also plan on using this site to host other poll aggregates, such as an updated generic ballot average, predictive models for elections, and potentially other statistical modeling projects unrelated to politics.</p>
        <p class="text-left" style="margin-left: 10%; margin-right: 10%">
            <b>NOTE</b>: The current generic ballot, congressional approval, and SCOTUS approval averages, as well as the presidential approval by pollster chart, are currently outdated and have not been updated. The generic ballot average will likely see an update in the foreseeable future, whereas congressional approval, SCOTUS approval, and presidential approval by pollster (in the Featured Charts tab) are archived and likely will not see further updates. Everything in the President tab is up-to-date and actively being updated.
        </p>
        <hr>
        <h1 class="text-center">Methodology</h1>
        <hr>
        <p class="text-left" style="margin-left: 10%; margin-right: 10%">
            SnoutCounter aggregates polling data via a weighted average. All scientific polls conducted by professional pollsters included in the relevant dataset are included in the aggregation, with the exception of polls conducted by <a href="about.html#bannedpollsters">:banned pollsters</a>. All presidential approval polls, both general and issue-specific, are collected manually. I generally check the <a href="https://www.natesilver.net/p/trump-approval-ratings-nate-silver-bulletin">Silver Bulletin</a>, <a href="https://fiftyplusone.news/polls/approval/president">FiftyPlusOne</a>, <a href="https://www.nytimes.com/interactive/polls/donald-trump-approval-rating-polls.html">The New York Times</a>, and <a href="https://bsky.app/profile/usapolling.bsky.social">Polling USA</a> for anything I missed. In the case that a poll has surveyed two samples of different population types (for example, one likely voter sample and one registered voter sample), we only use the results of one of these samples being surveyed. "All adult" samples are preferred to registered voter samples, and registered voter samples are preferred to likely voter samples, for the various job approval averages. When measuring job approval among registered voters, polls that draw from a sample of all adults, but include crosstab results for registered voters, are included - specifically the results for the registered voter sample. Tracking polls in our dataset are dynamically selected and weeded out such that all tracking polls from the same pollster are non-overlapping in fielding dates; I always include the most recent tracking poll from each pollster. Our weights are determined by the following four factors:
        </p>
        <ul class="text-left" style="margin-left: 10%; margin-right: 10%">
            <li><b>Sample size</b>: Polls with higher sample size are more likely to accurately estimate the population parameter in question, and generally have less uncertainty than polls with smaller sample sizes. However, sample sizes are subject to diminishing returns - a poll with a sample size of 5000 won't much more accurate than one with a sample size of 5000. I cap sample size at 5000, to avoid polls with particularly large sample sizes from dominating the averages. The weight function I use is the square root of the sample size over the square root of the median sample size of all polls in the dataset - this is similar to the function used by <a href="about.html#538">:538</a> for their averages before they shut down.</li>
            <li><b>Pollster rating</b>: Not all pollsters are created equal; some are more reliable in producing accurate and precise results than others. I use the <a href="https://www.natesilver.net/p/pollster-ratings-silver-bulletin">Silver Bulletin's pollster ratings</a>, specifically the predictive plus-minus, a measure of how accurate a pollster is expected to be, as the input for the pollster rating weight function. For predictive plus-minus, lower is better. Pollsters with a predictive plus-minus above 1 are assigned a flat weight of 0.2, as are pollsters without a predictive plus-minus rating from Silver Bulletin. All other pollsters are assigned a weight according to square root function.</li>
            <li><b>Time since poll was conducted</b>: Of course, polls which were conducted more recently are more likely to be reflective of the state of public opinion. I utilize an exponential function as a weight for recency, with the function being more aggressive with more frequently polled topics (e.g. general approval, versus issue-specific approval).</li>
            <li><b>Multiple polls in short window</b>: If there are multiple polls from the same pollster and sponsor in a two week window, each poll is downweighted, based on the number of polls from that pollster/sponsor in this window. This is to ensure that one pollster/sponsor doesn't dominate the averages just out of frequency, and to counter pollsters who attempt to "flood the zone." The formula utilized for this is borrowed from <a href="https://www.gelliottmorris.com/p/democrats-lead-house-generic-ballot">Strength In Numbers' polling averages</a>; it is calculated as 1 over the square root of the number of polls from the pollster/sponsor pair within a two-week window.</li>
        </ul>
        <p class="text-left" style="margin-left: 10%; margin-right: 10%">All these weights are combined into a final weight, calculated as the product of these four weights. The weights are normalized to sum to 1. This is then used to calculate a weighted average for each rating or variable we are trying to measure, and for each day. The weighted standard deviation is also calculated for each day and used to determine <a href="https://en.wikipedia.org/wiki/Confidence_interval">:confidence intervals</a>.</a></p>
        <p class="text-left" style="margin-left: 10%; margin-right: 10;">
            In addition to these weights, I apply four different adjustments to each poll result, each of them calculated by a <a href="https://bookdown.org/steve_midway/DAR/random-effects.html">random effects model</a>. These adjustments are calculated relative to the average calculated from the previous steps. The adjustments are as follows:
        </p>
        <ul class="text-left" style="margin-left: 10%; margin-right: 10%">
            <li><b>House effect adjustment</b>: Many pollsters have unique biases stemming from differences in fielding, weighting, question wording, etc, that may not be captured by other adjustments calculated. Thus, I calculate the "house effect" of each pollster. Like all other adjustments, house effects are calculated relative to the weighted average of all polls, rather than the biases of each pollster relative to election results - this is because systematic, industry-wide polling bias (and, by corollary, the biases of many pollsters relative to actual results) are not predictable and change from cycle to cycle.</li>
            <li><b>Mode adjustment</b>: The choice of how a pollster chooses to field will often have a significant effect on the results of the poll, as different fielding methods will often reach different audiences and different types of people. For example, online panels often reach a younger demographic compared to live phone surveys, and probability panels often present significantly different results compared to non-probability based methods. I correct for this by introducing an adjustment for methodology.</li>
            <li><b>Population adjustment</b>: Each polling average is trying to measure approval or horse-race standings for a specific population, but often we will have multiple surveys polling different populations for the same measurable variable. To address this, I calculate three population adjustments - likely voter, registered voter, and all adult sample adjustments - and adjust the results of polls surveying different populations towards the population that the average is trying to measure. For general and issue-specific presidential approval, the average attempts to measure approval among all American adults, while for approval among registered voters, the average attempts to measure approval among, well, registered voters. Thus, the model adjusts results towards the population being measured in each of these cases.</li>
            <li><b>Partisanship adjustment</b>: Some pollsters are very explicitly partisan in nature, often working with certain political parties or groups affiliated with political parties, consistently polling for candidates of one party, and/or being funded by a certain party or partisan group. For these pollsters, a partisanship adjustment is applied to correct for potential biases introduced by strong partisan affiliation. As an additional measure to counteract partisanship, I employ a <b>partisanship downweight</b> - all else held equal, a poll conducted by a partisan pollster is given 70% of the weight of a poll conducted by a non-partisan pollster.</li>
        </ul>
        <p class="text-left" style="margin-left: 10%; margin-right: 10%">
            These adjustments are summed to get the total adjustment for each poll, which is then halved and added/subtracted to measured approvals/disapprovals. After these adjustments are applied, the weighted average and confidence intervals are recalculated to get the final average for each rating/variable.
        </p>
        <h2 class="text-left" style="margin-left: 10%; margin-right: 10%">Updates</h2>
        <ul class="text-left" style="margin-left: 10%; margin-right: 10%">
            <li><b>January 8, 2026</b>: Added partisanship downweight.</li>
            <li><b>January 4, 2026</b>: Completely overhauled the methodology for polling averages.</li>
            <li><b>September 26, 2025</b>: Fixed bug in sample size weights where, in calculation of median sample size, poll sample sizes are capped at 2000 instead of the intended 3000.</li>
            <li><b>September 18, 2025</b>: Added a new issue to issue-approval poll averages: crime.</li>
            <li><b>September 16, 2025</b>: 
                <ul class="text-left">
                    <li>Polling averages now include an additional weight to account for multiple polls from the same pollster being conducted in a short window.</li>
                    <li>Unrated pollsters are now assigned a flat pollster quality weight of 0.2 instead of 0.1.</li>
                </ul>
            </li>
            <li><b>July 22, 2025</b>: Added a new issue to issue-approval poll averages: healthcare policy.</li>
            <li><b>July 21, 2025</b>: Added graphs measuring presidential job approval among registered voters. </li>
            <li><b>June 23, 2025</b>: Started including polls from pollsters without a Silver Bulletin pollster rating. For these pollsters, a flat pollster quality weight of 0.1 is assigned.</li>
            <li><b>June 21, 2025</b>: Unbanned McLaughlin, for similar reasons to the recent unbanning of other partisan pollsters. While McLaughlin is a particularly extreme case of partisanship, I am unaware of any significant methodological concerns beyond their bias, which can be rectified via house effect adjustment.</li>
            <li><b>June 20, 2025</b>: Unbanned OnMessage Inc. and North Star from use by SnoutCounter averages, for similar reasons to the unbanning of Civiqs and co/efficient.</li>
            <li><b>June 17, 2025</b>: 
                <ul class="text-left">
                    <li>Tweaked time weights for presidential approval polling (both overall and issue-specific) to be more aggressive. This should make the averages more responsive.</li>
                    <li>Unbanned co/efficient and Civiqs from use by SnoutCounter averages. These are both partisan pollsters, whose partisan bias is already largely rectified via house effect adjustment, and there really isn't much wrong with these polling outfits besides the aforementioned partisanship.</li>
                </ul>
            </li>
            <li><b>June 16, 2025</b>: Slightly tweaked population type weighting for generic ballot polling. This would lead to slight decrease in weights for polls utilizing all adult samples.</li>
            <li><b>June 15, 2025</b>: Added a "Featured Charts" section, so I can show other neat visualizations without cluttering up the main sections.</li>
            <li><b>May 31, 2025</b>: Modified code for calculating population weights for generic ballot polling averages. As generic ballot polling aims to measure voting intent, it makes more sense to value LV > RV > A.</li>
            <li><b>May 20, 2025</b>: Added a chart tracking net presidential approval rating.</li>
            <li><b>May 13, 2025</b>: Added a new issue to issue-approval poll averages: trade and tariffs.</li>
            <li><b>May 10, 2025</b>: Fixed bug in pipeline function that caused registered voter samples in polls to be chosen over all adult samples.</li>
            <li><b>April 19, 2025</b>: Added chart showcasing net issue-specific approval ratings.</li>
            <li><b>April 18, 2025</b>: Adjusted pollster quality weights to be slightly less aggressive, thus lowering the chance that an unduly small number of polls dominate the averages.</li>
            <li><b>April 17, 2025</b>: Adjusted the linear time weight to be somewhat more aggressive. This helps averages be more responsive and less sluggish, especially the Congressional and SCOTUS approval averages.</li>
        </ul>
        <h2 class="text-left" style="margin-left: 10%; margin-right: 10%">Data Download</h2>
        <p class="text-left" style="margin-left: 10%; margin-right: 10%">The polling data used for general presidential approval are available to download at <a href="https://docs.google.com/spreadsheets/d/e/2PACX-1vSj1f5S4BKqd_0T83atnX5WIXMhwd326O-jg5vXNjvylt8DIWNW07rXmMUy9wGEz5ZVDlfLbea_hrd9/pub?output=csv">this link</a>, while data for issue-specific presidential approval are available to download at <a href="https://docs.google.com/spreadsheets/d/e/2PACX-1vR7cOF5NSArcxNxYjzDjjTnFNmG-l0zM8WqabuCqNmwKke7VTEMKjR1BamqigAFeRCvbhCylaspQpTG/pub?gid=0&single=true&output=csv">this link</a>.</p>
        <hr>
        <h1 class="text-center">Acknowledgments & Support</h1>
        <hr>
        <h2 class="text-left" style="margin-left: 10%; margin-right: 10%">Data</h2>
        <p class="text-left" style="margin-left: 10%; margin-right: 10%">While the polling data is collected manually, the datasets collected by <a href="https://www.natesilver.net/p/trump-approval-ratings-nate-silver-bulletin">Silver Bulletin</a>, <a href="https://fiftyplusone.news/polls/approval/president">FiftyPlusOne</a>, <a href="https://www.nytimes.com/interactive/polls/donald-trump-approval-rating-polls.html">The New York Times</a>, <a href="https://www.realclearpolling.com/">RealClearPolitics</a>, and <a href="https://bsky.app/profile/usapolling.bsky.social">Polling USA</a> have made this work significantly easier for myself. Pollser ratings from the <a href="https://www.natesilver.net/p/pollster-ratings-silver-bulletin">Silver Bulletin</a>.</p>
        <h2 class="text-left" style="margin-left: 10%; margin-right: 10%">Site Design Tools</h2>
        <p class="text-left" style="margin-left: 10%; margin-right: 10%">
            Nutshell by <a href="https://ncase.me/nutshell/">Nicky Case</a>.
        </p>
        <p class="text-left" style="margin-left: 10%; margin-right: 10%">
            California Gothic font by <a href="https://www.mattlag.com/californiagothic/">Matt Lag</a>.
        </p>
        <p class="text-left" style="margin-left: 10%; margin-right: 10%;">
            IBM Plex Sans font can be found at <a href="https://fonts.google.com/specimen/IBM+Plex+Sans">Google Fonts</a>.
        </p>

        <h2 class="text-left" style="margin-left: 10%; margin-right: 10%">Support</h2>
        <p class="text-left" style="margin-left: 10%; margin-right: 10%">
            If you want to, you can throw money at me on <a href="https://ko-fi.com/hackquantumcpp">Ko-Fi</a>.</p>
        <!-- <h2 class="text-left" style="margin-left: 10%; margin-right: 10%">Other</h2> -->
        <!-- <p class="text-left" style="margin-left: 10%; margin-right: 10%">The <a href="https://education.github.com/pack">Github Student Developer Pack</a> for allowing me to do all this, including site hosting, for free.</p> -->

        <!-- Hidden sections -->
        <h2 class="text-left" style="margin-left: 10%; margin-right: 10%">:x banned pollsters</h2>
        <p class="text-left" style="margin-left: 10%; margin-right: 10%">
            Some polling outfits are banned from use by SnoutCounter for aggregation. The following pollsters are banned from SnoutCounter for methodological misconduct, lack of methodological transparency, and/or other methodological issues.
        </p>
        <ul class="text-left" style="margin-left: 10%; margin-right: 10%">
            <li>Rasmussen Reports</li>
            <li>Trafalgar Group</li>
            <li>TIPP Insights</li>
            <li>ActiVote</li>
        </ul>
        <p class="text-left" style="margin-left: 10%; margin-right: 10%">
            Additionally, the following pollsters are banned for having received an 'F' quality rating from the Silver Bulletin.
        </p>
        <ul class="text-left" style="margin-left: 10%; margin-right: 10%">
            <li>Strategic Vision LLC</li>
            <li>Pharos Research Group</li>
            <li>Research 2000</li>
            <li>Big Data Poll</li>
            <li>Overtime Politics</li>
            <li>Rethink Priorities</li>
            <li>Blumenthal Research Daily</li>
            <li>CSP Polling</li>
            <li>KG Polling</li>
            <li>OurProgress (The Progress Campaign)</li>
            <li>TCJ Research</li>
        </ul>

        <h2 class="text-left" style="margin-left: 10%; margin-right: 10%">:x 538</h2>
        <p class="text-left" style="margin-left: 10%; margin-right: 10%">RIP :(</p>
    </div>
<hr>
<footer class="bg-white text-black text-center">
    <p>SnoutCounter</p>
</footer>
</body>
</html>
